Could be interesting to 
generate text from different temperature settings: 
e.g. (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) and compare the 
sentence-level and the overall surprise distributions
that we get from this. 

We could do this for different genres;
e.g.
* write 500 word short story about x
* write 500 word article about x 
* write 500 word essay about x 
* write 500 word poem about x 

We could then have humans (or even GPT) grade these 
on a scale of 1-100 in terms of how good they are. 

The good thing would be if we could also compare them
to actual text within these different genres. Are there 
some patterns (at sentence level and/or at macro level)
that systematically differ? 

https://gptforwork.com/guides/openai-gpt3-temperature
For transformation tasks (extraction, standardization, 
format conversion, grammar fixes) 
prefer a temperature of 0 or up to 0.3.
For writing tasks, you should juice the temperature higher, 
closer to 0.5. If you want GPT to be highly creative 
(for marketing or advertising copy for instance), 
consider values between 0.7 and 1.